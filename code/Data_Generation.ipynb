{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS579: Online Social Network Analysis\n",
    "\n",
    "# Final Project - Recommendation Systems Using Map-Reduce\n",
    "\n",
    "\n",
    "$$J V P S Avinash $$ <br>\n",
    "$$Rakshith Muniraju $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part - 1 : Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains the data generation code to extract the data from Rotten Tomatoes Movie Data Base. <br>\n",
    "The data generation contains two steps: <br>\n",
    "1) Connect to Rotten Tomatoes API to get the top 100 movies from the last two years. <br>\n",
    "2) Implement a Web Crawler to extract the User-Rating information from the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from BeautifulSoup import BeautifulSoup\n",
    "import sys\n",
    "import re\n",
    "import urllib\n",
    "import urllib2\n",
    "import urlparse\n",
    "import io\n",
    "import rtsimple as rt\n",
    "from unidecode import unidecode\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MyOpener(urllib.FancyURLopener):\n",
    "    version = 'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.2.15) Gecko/20110303 Firefox/3.6.15'\n",
    "    Connection= 'keep-alive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rt.API_KEY = 'usdusq9ana3aq2r637nw5572'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My Opener function helps in connecting to the Firefox to implemet Web Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ALL_URLs_TOP_RT_2015 = list()\n",
    "ALL_URLs_TOP_RT_2014 = list()\n",
    "myopener = MyOpener()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following couple cells of codes contains the information of how to connect to the Rotten Tomatoes site to \n",
    "extract the information of top 100 movies from the years 2015 and 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PAGE_URL_TOP_RT = \"http://www.rottentomatoes.com/top/bestofrt/?year=2015\"\n",
    "soup = BeautifulSoup(myopener.open(PAGE_URL_TOP_RT).read())\n",
    "if soup.find('title').contents[0] == \"Page Not Found\":\n",
    "    print \"Not Found\"\n",
    "else:\n",
    "    all_movies_rt = soup.findAll('table',{'class':'table'})\n",
    "    all_movies_rt2 = all_movies_rt[0].findChildren('a',{'class':'unstyled articleLink'})\n",
    "for i3 in range(0,len(all_movies_rt2)):\n",
    "    ALL_URLs_TOP_RT_2015.append('http://www.rottentomatoes.com'+all_movies_rt2[i3]['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PAGE_URL_TOP_RT = \"http://www.rottentomatoes.com/top/bestofrt/?year=2014\"\n",
    "soup = BeautifulSoup(myopener.open(PAGE_URL_TOP_RT).read())\n",
    "if soup.find('title').contents[0] == \"Page Not Found\":\n",
    "    print \"Not Found\"\n",
    "else:\n",
    "    all_movies_rt = soup.findAll('table',{'class':'table'})\n",
    "    all_movies_rt2 = all_movies_rt[0].findChildren('a',{'class':'unstyled articleLink'})\n",
    "for i3 in range(0,len(all_movies_rt2)):\n",
    "    ALL_URLs_TOP_RT_2014.append('http://www.rottentomatoes.com'+all_movies_rt2[i3]['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SUB_URL   = '/reviews/?page='\n",
    "END_URL   = \"&type=user&sort=\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the API uses stars for ratings and a number of 0.5 for partial ratings, we need to write a function to\n",
    "calculate rating for those users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_rating(rating):\n",
    "    number_of_ratings = len(rating) - 1 #5\n",
    "    full_ratings = rating.findAll('span') #4\n",
    "    half_ratings = number_of_ratings - len(full_ratings) #1\n",
    "    return len(full_ratings) + (half_ratings*0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell code crawls over the web site to extract the information about the ratings given by user in 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fo = open('output_v2', 'w')\n",
    "count = 0\n",
    "for movie_urls in ALL_URLs_TOP_RT_2015:\n",
    "    for page_number in range(1,6):\n",
    "        URL = movie_urls + SUB_URL + str(page_number) + END_URL\n",
    "        print URL\n",
    "        \n",
    "        soup = BeautifulSoup(myopener.open(URL).read())\n",
    "        if soup.find('title').contents[0] == \"Page Not Found\":\n",
    "            print \"Not Found\"\n",
    "        all_users = soup.findAll('a', {'class':'bold unstyled articleLink'})\n",
    "        all_ratings_div = soup.findChildren('div',{'class':'col-xs-16'})\n",
    "        print len(all_users),len(all_ratings_div)\n",
    "        \n",
    "        for user in range(0,len(all_ratings_div)):\n",
    "            count = count + 1\n",
    "            all_ratings = all_ratings_div[user].findChildren('span', {'class':'fl'})\n",
    "            if len(all_ratings) == 0:\n",
    "                fo.write('|'.join([unidecode(all_users[user].contents[0]), movie_urls[32:len(movie_urls)-1] , str(0)]) + '\\n')\n",
    "            else:\n",
    "                rating = calculate_rating(all_ratings[0])\n",
    "                if len(all_users[user].contents) == 0:\n",
    "                    fo.write('|'.join([\"rt_user_anonymus_\"+str(count), movie_urls[32:len(movie_urls)-1] , str(rating)]) + '\\n')\n",
    "                else:\n",
    "                    #print all_users[user].contents[0]\n",
    "                    fo.write('|'.join([unidecode(all_users[user].contents[0]), movie_urls[32:len(movie_urls)-1] , str(rating)]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell code crawls over the web site to extract the information about the ratings given by user in 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fo = open('output_v3', 'w')\n",
    "count = 0\n",
    "for movie_urls in ALL_URLs_TOP_RT_2014:\n",
    "    for page_number in range(1,6):\n",
    "        URL = movie_urls + SUB_URL + str(page_number) + END_URL\n",
    "        print URL\n",
    "        \n",
    "        soup = BeautifulSoup(myopener.open(URL).read())\n",
    "        if soup.find('title').contents[0] == \"Page Not Found\":\n",
    "            print \"Not Found\"\n",
    "        all_users = soup.findAll('a', {'class':'bold unstyled articleLink'})\n",
    "        all_ratings_div = soup.findChildren('div',{'class':'col-xs-16'})\n",
    "        print len(all_users),len(all_ratings_div)\n",
    "        \n",
    "        for user in range(0,len(all_ratings_div)):\n",
    "            count = count + 1\n",
    "            all_ratings = all_ratings_div[user].findChildren('span', {'class':'fl'})\n",
    "            if len(all_ratings) == 0:\n",
    "                fo.write('|'.join([unidecode(all_users[user].contents[0]), movie_urls[32:len(movie_urls)-1] , str(0)]) + '\\n')\n",
    "            else:\n",
    "                rating = calculate_rating(all_ratings[0])\n",
    "                if len(all_users[user].contents) == 0:\n",
    "                    fo.write('|'.join([\"rt_user_anonymus_\"+str(count), movie_urls[32:len(movie_urls)-1] , str(rating)]) + '\\n')\n",
    "                else:\n",
    "                    #print all_users[user].contents[0]\n",
    "                    fo.write('|'.join([unidecode(all_users[user].contents[0]), movie_urls[32:len(movie_urls)-1] , str(rating)]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We merge two files into a single file by opening the shell of python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subprocess.Popen('cat output_v2 output_v3 > ratings_data',shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kevin M. W|mad_max_fury_road|4.5\r\n",
      "familiar s|mad_max_fury_road|2.5\r\n",
      "Phil H|mad_max_fury_road|3.5\r\n",
      "Wildaly M|mad_max_fury_road|4.0\r\n",
      "Kase V|mad_max_fury_road|5.0\r\n",
      "RCCLBC|mad_max_fury_road|4.0\r\n",
      "Chris G|mad_max_fury_road|5.0\r\n",
      "Al S|mad_max_fury_road|5.0\r\n",
      "Carlos M|mad_max_fury_road|4.5\r\n",
      "Eugene B|mad_max_fury_road|4.5\r\n"
     ]
    }
   ],
   "source": [
    "!head ratings_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14374 ratings_data\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l ratings_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the demo, we are using the sample file which is randomly downloaded over the net.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jack Matthews|Lady in the Water|3.000000\r\n",
      "Jack Matthews|Snakes on a Plane|4.000000\r\n",
      "Jack Matthews|You, Me and Dupree|3.500000\r\n",
      "Jack Matthews|Superman Returns|5.000000\r\n",
      "Jack Matthews|The Night Listener|3.000000\r\n",
      "Mick LaSalle|Lady in the Water|3.000000\r\n",
      "Mick LaSalle|Snakes on a Plane|4.000000\r\n",
      "Mick LaSalle|Just My Luck|2.000000\r\n",
      "Mick LaSalle|Superman Returns|3.000000\r\n",
      "Mick LaSalle|You, Me and Dupree|2.000000\r\n"
     ]
    }
   ],
   "source": [
    "!head samp.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 samp.csv\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l samp.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
